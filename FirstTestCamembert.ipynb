{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c18668",
   "metadata": {},
   "source": [
    "####  Load CamemBERT and its sub-word tokenizer : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5e33b280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertModel, CamembertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ada5586",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert/camembert-base-wikipedia-4gb were not used when initializing CamembertModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CamembertModel(\n",
       "  (embeddings): CamembertEmbeddings(\n",
       "    (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): CamembertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x CamembertLayer(\n",
       "        (attention): CamembertAttention(\n",
       "          (self): CamembertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): CamembertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): CamembertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): CamembertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): CamembertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# You can replace \"camembert-base\" with any other model from the table, e.g. \"camembert/camembert-large\".\n",
    "tokenizer = CamembertTokenizer.from_pretrained(\"camembert/camembert-base-wikipedia-4gb\")\n",
    "camembert = CamembertModel.from_pretrained(\"camembert/camembert-base-wikipedia-4gb\")\n",
    "\n",
    "camembert.eval()  # disable dropout (or leave in train mode to finetune)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda87202",
   "metadata": {},
   "source": [
    "#### Filling masks using pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cdf017c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.4937813878059387,\n",
       "  'token': 19370,\n",
       "  'token_str': 'chèvre',\n",
       "  'sequence': 'Le camembert est un fromage de chèvre!'},\n",
       " {'score': 0.06255923956632614,\n",
       "  'token': 30616,\n",
       "  'token_str': 'brebis',\n",
       "  'sequence': 'Le camembert est un fromage de brebis!'},\n",
       " {'score': 0.043401967734098434,\n",
       "  'token': 2364,\n",
       "  'token_str': 'montagne',\n",
       "  'sequence': 'Le camembert est un fromage de montagne!'},\n",
       " {'score': 0.02823261171579361,\n",
       "  'token': 3236,\n",
       "  'token_str': 'Noël',\n",
       "  'sequence': 'Le camembert est un fromage de Noël!'},\n",
       " {'score': 0.02135733887553215,\n",
       "  'token': 12329,\n",
       "  'token_str': 'vache',\n",
       "  'sequence': 'Le camembert est un fromage de vache!'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline \n",
    "\n",
    "camembert_fill_mask  = pipeline(\"fill-mask\", model=\"camembert/camembert-base-wikipedia-4gb\", tokenizer=\"camembert/camembert-base-wikipedia-4gb\")\n",
    "results = camembert_fill_mask(\"Le camembert est un fromage de <mask>!\")\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdeea72",
   "metadata": {},
   "source": [
    "####  Extract contextual embedding features from Camembert output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "74775b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Tokenize in sub-words with SentencePiece\n",
    "tokenized_sentence = tokenizer.tokenize(\"J'aime le camembert !\")\n",
    "# ['▁J', \"'\", 'aime', '▁le', '▁ca', 'member', 't', '▁!'] \n",
    "\n",
    "# 1-hot encode and add special starting and end tokens \n",
    "encoded_sentence = tokenizer.encode(tokenized_sentence)\n",
    "# [5, 221, 10, 10600, 14, 8952, 10540, 75, 1114, 6]\n",
    "# NB: Can be done in one step : tokenize.encode(\"J'aime le camembert !\")\n",
    "\n",
    "# Feed tokens to Camembert as a torch tensor (batch dim 1)\n",
    "encoded_sentence = torch.tensor(encoded_sentence).unsqueeze(0)\n",
    "embeddings = camembert(encoded_sentence)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ab473255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0928,  0.0506, -0.0094,  ..., -0.2388,  0.1177, -0.1302],\n",
       "         [ 0.0662,  0.1030, -0.2355,  ..., -0.4224, -0.0574, -0.2802],\n",
       "         [-0.0729,  0.0547,  0.0192,  ..., -0.1743,  0.0998, -0.2677],\n",
       "         ...,\n",
       "         [-0.0033, -0.1228, -0.2961,  ...,  0.3828,  0.0659,  0.2365],\n",
       "         [ 0.1366,  0.2483,  0.1271,  ...,  0.1985, -0.1444, -0.4721],\n",
       "         [ 0.0778,  0.1016,  0.0436,  ...,  0.0237, -0.1466, -0.1009]]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f168361",
   "metadata": {},
   "source": [
    "####  Extract contextual embedding features from all Camembert layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "97727820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CamembertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8f76334b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert/camembert-base-wikipedia-4gb were not used when initializing CamembertModel: ['lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing CamembertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# (Need to reload the model with new config)\n",
    "config = CamembertConfig.from_pretrained(\"camembert/camembert-base-wikipedia-4gb\", output_hidden_states=True)\n",
    "camembert = CamembertModel.from_pretrained(\"camembert/camembert-base-wikipedia-4gb\", config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "60bfe64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0059, -0.0227,  0.0065,  ..., -0.0770,  0.0369,  0.0095],\n",
       "         [ 0.2838, -0.1531, -0.3642,  ..., -0.0027, -0.8502, -0.7914],\n",
       "         [-0.0073, -0.0338, -0.0011,  ...,  0.0533, -0.0250, -0.0061],\n",
       "         ...,\n",
       "         [-0.1932,  0.0468,  0.2520,  ...,  0.8156, -0.4552,  0.2495],\n",
       "         [ 0.2540,  0.9947,  0.4313,  ..., -0.3552, -0.0192, -1.1114],\n",
       "         [ 0.1387,  0.2407,  0.0506,  ..., -0.6924,  0.2260, -0.6020]]],\n",
       "       grad_fn=<NativeLayerNormBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = camembert(encoded_sentence)[0]\n",
    "all_layer_embeddings = camembert(encoded_sentence)[2]\n",
    "#  all_layer_embeddings list of len(all_layer_embeddings) == 13 (input embedding layer + 12 self attention layers)\n",
    "all_layer_embeddings[5]\n",
    "# layer 5 contextual embedding : size torch.Size([1, 10, 768])\n",
    "#tensor([[[-0.0059, -0.0227,  0.0065,  ..., -0.0770,  0.0369,  0.0095],\n",
    "#         [ 0.2838, -0.1531, -0.3642,  ..., -0.0027, -0.8502, -0.7914],\n",
    "#         [-0.0073, -0.0338, -0.0011,  ...,  0.0533, -0.0250, -0.0061],\n",
    "#         ...,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca1dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings{}"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
