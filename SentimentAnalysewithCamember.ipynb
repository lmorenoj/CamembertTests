{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9619bae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import seaborn\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import CamembertModel, CamembertForSequenceClassification, CamembertTokenizer, AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b224aaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement du jeu de donnees\n",
    "dataset = pd.read_csv(\"reviews_allocine_classification.csv\")\n",
    "reviews = dataset['review'].values.tolist()\n",
    "sentiments = dataset['sentiment'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bea80ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca128324cd884bec8b0e18987e3b8dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/811k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2564f066f07467297fc96594aaf0fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/508 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# On charge l'objet \"tokenizer\"de camemBERT qui va servir a encoder\n",
    "# 'camebert-base' est la version de camembert qu'on choisit d'utiliser\n",
    "# 'do_lower_case' à True pour qu'on passe tout en miniscule\n",
    "TOKENIZER = CamembertTokenizer.from_pretrained(\n",
    "    'camembert-base',\n",
    "    do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "99a8b503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# La fonction batch_encode_plus encode un batch de donnees\n",
    "encoded_batch = TOKENIZER.batch_encode_plus(reviews,\n",
    "                                            add_special_tokens=True,\n",
    "                                            max_length=25,\n",
    "                                            padding=True,\n",
    "                                            truncation=True,\n",
    "                                            return_attention_mask = True,\n",
    "                                            return_tensors = 'pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "67c97d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_124613/2557083801.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  sentiments = torch.tensor(sentiments)\n"
     ]
    }
   ],
   "source": [
    "# On transforme la liste des sentiments en tenseur\n",
    "sentiments = torch.tensor(sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3c477bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On calcule l'indice qui va delimiter nos datasets d'entrainement et de validation\n",
    "# On utilise 80% du jeu de donnée pour l'entrainement et les 20% restant pour la validation\n",
    "split_border = int(len(sentiments)*0.8)\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    encoded_batch['input_ids'][:split_border],\n",
    "    encoded_batch['attention_mask'][:split_border],\n",
    "    sentiments[:split_border])\n",
    "validation_dataset = TensorDataset(\n",
    "    encoded_batch['input_ids'][split_border:],\n",
    "    encoded_batch['attention_mask'][split_border:],\n",
    "    sentiments[split_border:])\n",
    " \n",
    " \n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "8155cc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On cree les DataLoaders d'entrainement et de validation\n",
    "# Le dataloader est juste un objet iterable\n",
    "# On le configure pour iterer le jeu d'entrainement de façon aleatoire et creer les batchs.\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset),\n",
    "            batch_size = batch_size)\n",
    " \n",
    "validation_dataloader = DataLoader(\n",
    "            validation_dataset,\n",
    "            sampler = SequentialSampler(validation_dataset),\n",
    "            batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e17534",
   "metadata": {},
   "source": [
    "## Chargement du modèle\n",
    "\n",
    "Cela se fait en une ligne de code grâce à Transformers qui a déjà implémenté le fine-tuning pour nous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "027391e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert-base were not used when initializing CamembertForSequenceClassification: ['lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# On la version pre-entrainee de camemBERT 'base'\n",
    "model = CamembertForSequenceClassification.from_pretrained(\n",
    "    'camembert-base',\n",
    "    num_labels = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c23af3",
   "metadata": {},
   "source": [
    "## Hyperparamètres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "99b8bfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                  lr = 2e-5, # Learning Rate\n",
    "                  eps = 1e-8 # Epsilon\n",
    "                 )\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8a0200",
   "metadata": {},
   "source": [
    "## Entraînement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7ab90ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "########## Epoch 1 / 3 ##########\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.66\n",
      "\n",
      "########## Epoch 2 / 3 ##########\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.65\n",
      "\n",
      "########## Epoch 3 / 3 ##########\n",
      "Training...\n",
      "\n",
      "  Average training loss: 0.64\n",
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "# On va stocker nos tensors sur mon cpu : je n'ai pas mieux\n",
    "device = torch.device(\"cpu\")\n",
    " \n",
    "# Pour enregistrer les stats a chaque epoque\n",
    "training_stats = []\n",
    " \n",
    "# Boucle d'entrainement\n",
    "for epoch in range(0, epochs):\n",
    "     \n",
    "    print(\"\")\n",
    "    print(f'########## Epoch {epoch+1} / {epochs} ##########')\n",
    "    print('Training...')\n",
    " \n",
    " \n",
    "    # On initialise la loss pour cette epoque\n",
    "    total_train_loss = 0\n",
    " \n",
    "    # On met le modele en mode 'training'\n",
    "    # Dans ce mode certaines couches du modele agissent differement\n",
    "    model.train()\n",
    " \n",
    "    # Pour chaque batch\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    " \n",
    "        # On fait un print chaque 40 batchs\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            print('f  Batch {step}  of {len(train_dataloader)}.')\n",
    "         \n",
    "        # On recupere les donnees du batch\n",
    "        input_id = batch[0].to(device)\n",
    "        attention_mask = batch[1].to(device)\n",
    "        sentiment = batch[2].to(device)\n",
    " \n",
    "        # On met le gradient a 0\n",
    "        model.zero_grad()        \n",
    " \n",
    "        # On passe la donnee au model et on recupere la loss et le logits (sortie avant fonction d'activation)\n",
    "        data = model(input_id, \n",
    "                             token_type_ids=None, \n",
    "                             attention_mask=attention_mask, \n",
    "                             labels=sentiment)\n",
    " \n",
    "        # On incremente la loss totale\n",
    "        # .item() donne la valeur numerique de la loss\n",
    "        total_train_loss += data['loss']\n",
    " \n",
    "        # Backpropagtion\n",
    "        data['loss'].backward()\n",
    " \n",
    "        # On actualise les parametrer grace a l'optimizer\n",
    "        optimizer.step()\n",
    " \n",
    "    # On calcule la  loss moyenne sur toute l'epoque\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)   \n",
    " \n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))  \n",
    "     \n",
    "    # Enregistrement des stats de l'epoque\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "        }\n",
    "    )\n",
    " \n",
    "print(\"Model saved!\")\n",
    "torch.save(model.state_dict(), \"./sentiments.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449d4d15",
   "metadata": {},
   "source": [
    "## Évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "207aba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(raw_reviews, sentiments=None):\n",
    "    encoded_batch = TOKENIZER.batch_encode_plus(raw_reviews,\n",
    "                                                truncation=True,\n",
    "                                                pad_to_max_length=True,\n",
    "                                                return_attention_mask=True,\n",
    "                                                return_tensors = 'pt')\n",
    "    if sentiments:\n",
    "        sentiments = torch.tensor(sentiments)\n",
    "        return encoded_batch['input_ids'], encoded_batch['attention_mask'], sentiments\n",
    "    return encoded_batch['input_ids'], encoded_batch['attention_mask']\n",
    " \n",
    "def predict(reviews, model=model):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        input_ids, attention_mask = preprocess(reviews)\n",
    "        retour = model(input_ids, attention_mask=attention_mask)\n",
    "         \n",
    "        return torch.argmax(retour[0], dim=1)\n",
    "\n",
    "def evaluate(reviews, sentiments):\n",
    "    predictions = predict(reviews)\n",
    "    print(metrics.f1_score(sentiments, predictions, average='weighted', zero_division=0))\n",
    "    seaborn.heatmap(metrics.confusion_matrix(sentiments, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2d132bbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luis/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2364: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6805555555555557\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "evaluate(reviews,sentiments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
